{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":708,"status":"ok","timestamp":1676469964459,"user":{"displayName":"Longtao Zheng","userId":"17721972102166804099"},"user_tz":-480},"id":"ehHMJ829mzJc"},"outputs":[],"source":["from typing import List, Tuple\n","import copy\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"UHqph9kA6wH7"},"source":["The following class defines the grid world environment. The grid world looks like:\n","\n","```\n","____________________________________________________________\n","|___|_0_|_1_|_2_|_3_|_4_|_5_|_6_|_7_|_8_|_9_|_10|_11|_12|_13|\n","|_0_|___|___|___|___|___|___|_x_|_x_|___|___|___|___|___|___|\n","|_1_|___|___|___|___|___|___|_x_|_x_|___|___|___|___|___|___|\n","|_2_|___|___|___|_x_|___|___|_x_|_x_|___|___|___|___|_x_|___|\n","|_3_|___|___|___|_x_|___|___|_x_|___|___|___|___|_x_|_x_|___|\n","|_4_|___|_B_|___|_x_|___|___|___|___|___|___|___|_x_|_x_|_G_|\n","|_5_|_A_|___|___|_x_|___|___|___|___|___|___|___|_x_|_x_|___|\n","```\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":516,"status":"ok","timestamp":1676469970811,"user":{"displayName":"Longtao Zheng","userId":"17721972102166804099"},"user_tz":-480},"id":"-RMDF0A3oKfX"},"outputs":[],"source":["AGENT = 'A'\n","BOX = 'B'\n","GOAL = 'G'\n","DANGER = 'x'\n","GRID = '_'\n","\n","\n","class CliffBoxGridWorld:\n","    \"\"\"\n","    Cliff Box Pushing Grid World.\n","    \"\"\"\n","    action_space = [1, 2, 3, 4]\n","    forces = {\n","        1: np.array([-1, 0]),\n","        2: np.array([1,  0]),\n","        3: np.array([0, -1]),\n","        4: np.array([0,  1]),\n","    }\n","    world_width = 14\n","    world_height = 6\n","    goal_pos = np.array([4, 13])\n","    init_agent_pos = np.array([5, 0])\n","    init_box_pos = np.array([4, 1])\n","    danger_region = [\n","        [(2, 3), (5, 3)],\n","        [(0, 6), (3, 6)],\n","        [(0, 7), (2, 7)],\n","        [(3, 11), (5, 11)],\n","        [(2, 12), (5, 12)],\n","    ]\n","\n","    def __init__(self,\n","                 episode_length=100,\n","                 render=False,\n","                 ):\n","        \"\"\"\n","        The grid world looks like:\n","        ____________________________________________________________\n","        |___|_0_|_1_|_2_|_3_|_4_|_5_|_6_|_7_|_8_|_9_|_10|_11|_12|_13|\n","        |_0_|___|___|___|___|___|___|_x_|_x_|___|___|___|___|___|___|\n","        |_1_|___|___|___|___|___|___|_x_|_x_|___|___|___|___|___|___|\n","        |_2_|___|___|___|_x_|___|___|_x_|_x_|___|___|___|___|_x_|___|\n","        |_3_|___|___|___|_x_|___|___|_x_|___|___|___|___|_x_|_x_|___|\n","        |_4_|___|_B_|___|_x_|___|___|___|___|___|___|___|_x_|_x_|_G_|\n","        |_5_|_A_|___|___|_x_|___|___|___|___|___|___|___|_x_|_x_|___|\n","        \"\"\"\n","        # Environment configurations.\n","        self.episode_length = episode_length\n","        self.render = render\n","        self.agent_pos = self.init_agent_pos\n","        self.box_pos = self.init_box_pos\n","\n","        # Visualization.\n","        if self.render:\n","            self.world = np.chararray((self.world_height, self.world_width))\n","            self.last_agent_pos = copy.deepcopy(self.agent_pos)\n","            self.last_box_pos = copy.deepcopy(self.box_pos)\n","            self.world[:] = GRID\n","            for region in self.danger_region:\n","                A, B = region\n","                assert A[1] == B[1], \"A[1] != B[1]\"\n","                self.world[A[0]:B[0]+1, A[1]] = DANGER\n","            self.world[self.agent_pos[0], self.agent_pos[1]] = AGENT\n","            self.world[self.box_pos[0], self.box_pos[1]] = BOX\n","            self.world[self.goal_pos[0], self.goal_pos[1]] = GOAL\n","\n","    def reset(self):\n","        \"\"\"\n","        Resets the environment.\n","\n","        Returns:\n","            The initial state (agent position and box position).\n","        \"\"\"\n","        self.timesteps = 0\n","        self.action_history = []\n","        self.agent_pos = self.init_agent_pos\n","        self.box_pos = self.init_box_pos\n","\n","        return tuple([*self.agent_pos.tolist(), *self.box_pos.tolist()])\n","\n","    def step(self, actions: int):\n","        \"\"\" \n","        Args: actions (a list of int).\n","\n","        Returns:\n","            The next state, reward, done, info.\n","        \"\"\"\n","        self.action_history.append(actions)\n","\n","        # Update the state.\n","        force = self.forces[actions]\n","        # check if the agent is near the box\n","        if np.sum(np.abs(self.agent_pos - self.box_pos)) == 1:\n","            # check if box is moved\n","            if all(self.agent_pos + force == self.box_pos):\n","                # check out of boundary\n","                self.box_pos = self._check_pos_boundary(pos=self.box_pos + force, box_hard_boundary=True)\n","        # move the agent\n","        new_agent_pos = self._check_pos_boundary(self.agent_pos + force)\n","        if not all(new_agent_pos == self.box_pos):\n","            self.agent_pos = new_agent_pos\n","        state = tuple([*self.agent_pos.tolist(), *self.box_pos.tolist()])\n","\n","        # Calculate the rewards\n","        done = self.timesteps == self.episode_length - 1\n","        # the distance between agents and box\n","        dist = np.sum(np.abs(self.agent_pos - self.box_pos))\n","        reward = -1  # -1 for each step\n","        reward -= dist\n","        # if agents or box is off the cliff\n","        if self._check_off_cliff(self.agent_pos) or self._check_off_cliff(self.box_pos):\n","            reward += -1000\n","            done = True\n","        \n","        if all(self.box_pos == self.goal_pos):\n","            reward += 1000\n","            done = True\n","        \n","        reward -= np.sum(np.abs(self.box_pos - self.goal_pos))\n","\n","        if self.render:\n","            self._update_render()\n","\n","        self.timesteps += 1\n","        info = {}\n","\n","        return state, reward, done, info\n","\n","    def print_world(self):\n","        \"\"\"\n","        Render the world in the command line.\n","        \"\"\"\n","        if len(self.action_history) > 0:\n","            print(f'Action: {self.action_history[-1]}')\n","        print(self.world)\n","\n","    def _check_pos_boundary(self, pos, box_hard_boundary: bool = False):\n","        \"\"\"\n","        Move the given position within the world bound.\n","        \"\"\"\n","        if pos[0] < 0:\n","            pos[0] = 0\n","        if pos[0] >= self.world_height:\n","            pos[0] = self.world_height - 1\n","        if pos[1] < 0:\n","            pos[1] = 0\n","        if pos[1] >= self.world_width:\n","            pos[1] = self.world_width - 1\n","        \n","        if box_hard_boundary:\n","            if pos[0] == 0:\n","                pos[0] += 1\n","            elif pos[0] == self.world_height - 1:\n","                pos[0] = self.world_height - 2\n","            if pos[1] == 0:\n","                pos[1] += 1\n","                    \n","        return pos\n","\n","    def _check_off_cliff(self, pos):\n","        \"\"\"\n","        Check if the given position is off cliff.\n","        \"\"\"\n","        for region in self.danger_region:\n","            A, B = region\n","            assert A[1] == B[1], \"A[1] != B[1]\"\n","            if A[0] <= pos[0] <= B[0] and pos[1] == A[1]:\n","                return True\n","        return False\n","\n","    def _update_render(self):\n","        \"\"\"\n","        Update the render information.\n","        \"\"\"\n","        if not all(self.last_agent_pos == self.agent_pos):\n","                pos = self.last_agent_pos\n","                if (pos[0] != self.goal_pos[0]) or (pos[1] != self.goal_pos[1]):\n","                    self.world[pos[0], pos[1]] = GRID\n","\n","        if not all(self.last_box_pos == self.box_pos):\n","            pos = self.last_box_pos\n","            if self.world[pos[0], pos[1]].decode('UTF-8') not in {AGENT}:\n","                self.world[pos[0], pos[1]] = GRID\n","\n","        if (self.agent_pos[0] != self.goal_pos[0]) or (self.agent_pos[1] != self.goal_pos[1]):\n","            self.world[self.agent_pos[0], self.agent_pos[1]] = AGENT\n","        self.world[self.box_pos[0], self.box_pos[1]] = BOX\n","        self.last_box_pos = copy.deepcopy(self.box_pos)\n","        self.last_agent_pos = copy.deepcopy(self.agent_pos)"]},{"cell_type":"markdown","metadata":{"id":"GKUvcFzIY4IX"},"source":["Here is one example random agent class:"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":348,"status":"ok","timestamp":1676469978101,"user":{"displayName":"Longtao Zheng","userId":"17721972102166804099"},"user_tz":-480},"id":"MjhmwF8VpVki"},"outputs":[],"source":["class RandomAgent:\n","    def __init__(self, env, num_episodes):\n","        self.action_space = [1, 2, 3, 4]\n","        self.env = env\n","        self.num_episodes = num_episodes\n","\n","    def act(self):\n","        \"\"\"Returns a random choice of the available actions\"\"\"\n","        return np.random.choice(self.action_space)\n","\n","    def learn(self):\n","        rewards = []\n","        \n","        for _ in range(self.num_episodes):\n","            cumulative_reward = 0 # Initialise values of each game\n","            state = self.env.reset()\n","            done = False\n","            while not done: # Run until game terminated\n","                action = self.act() \n","                next_state, reward, done, info = self.env.step(action)\n","                cumulative_reward += reward\n","                state = next_state\n","            rewards.append(cumulative_reward)\n","\n","        return rewards"]},{"cell_type":"markdown","metadata":{"id":"QYpyzvV8Y7j6"},"source":["You need to complete the learn() method of the following class to implement your RL algorithm."]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1676463847348,"user":{"displayName":"Longtao Zheng","userId":"17721972102166804099"},"user_tz":-480},"id":"vAKVvHmOptDW"},"outputs":[],"source":["class RLAgent:\n","    def __init__(self, env, num_episodes, epsilon=0.1, alpha=0.1, gamma=0.99):\n","        self.action_space = env.action_space\n","        self.q_table = dict() # Store all Q-values in a dictionary\n","        # Loop through all possible grid spaces, create sub-dictionary for each\n","        for agent_x in range(env.world_height):\n","            for agent_y in range(env.world_width):\n","                for box_x in range(env.world_height):\n","                    for box_y in range(env.world_width):\n","                        # Populate sub-dictionary with zero values for possible moves\n","                        self.q_table[(agent_x, agent_y, box_x, box_y)] = {k: 0 for k in self.action_space}\n","\n","        self.env = env\n","        self.num_episodes = num_episodes\n","        self.epsilon = epsilon\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        \n","    def act(self, state):\n","        \"\"\"Returns the (epsilon-greedy) optimal action from Q-Value table.\"\"\"\n","        if np.random.uniform(0,1) < self.epsilon:\n","            action = self.action_space[np.random.randint(0, len(self.action_space))]\n","        else:\n","            q_values_of_state = self.q_table[state]\n","            maxValue = max(q_values_of_state.values())\n","            action = np.random.choice([k for k, v in q_values_of_state.items() if v == maxValue])\n","        \n","        return action\n","\n","    def learn(self):\n","        \"\"\"Updates Q-values iteratively.\"\"\"\n","        rewards = []\n","        \n","        for _ in range(self.num_episodes):\n","            cumulative_reward = 0 # Initialise values of each game\n","            state = self.env.reset()\n","            done = False\n","            while not done: # Run until game terminated\n","                raise NotImplementedError\n","                # TODO: Update Q-values\n","\n","            rewards.append(cumulative_reward)\n","\n","        return rewards"]},{"cell_type":"markdown","metadata":{"id":"T4Zp2jCTZEgw"},"source":["Here is the game interface where you can manually move the agent."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10398,"status":"ok","timestamp":1676463857733,"user":{"displayName":"Longtao Zheng","userId":"17721972102166804099"},"user_tz":-480},"id":"OK-cjWJA7ymv","outputId":"5ed9e3b2-c2de-4808-d867-e2cb04c9df21"},"outputs":[],"source":["env = CliffBoxGridWorld(render=True)\n","env.reset()\n","env.print_world()\n","done = False\n","rewards = []\n","\n","while not done:\n","    action = int(input(\"Please input the actions (up: 1, down: 2, left: 3, right: 4): \"))\n","    state, reward, done, info = env.step(action)\n","    rewards.append(reward)\n","    print(f'step: {env.timesteps}, state: {state}, actions: {action}, reward: {reward}')\n","    env.print_world()\n","\n","print(f'rewards: {sum(rewards)}')\n","print(f'action history: {env.action_history}')"]},{"cell_type":"markdown","metadata":{"id":"wGyOJYn0ZJf9"},"source":["Example code to step random agent in the environment."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"executionInfo":{"elapsed":947,"status":"ok","timestamp":1676470008831,"user":{"displayName":"Longtao Zheng","userId":"17721972102166804099"},"user_tz":-480},"id":"d6ko9XBZ7z_z","outputId":"20c40233-27ff-4794-e8b0-9c290025c46f"},"outputs":[],"source":["# Initialize the environment and agent\n","env = CliffBoxGridWorld()\n","agent = RandomAgent(env, num_episodes=1000)\n","rewards = agent.learn()\n","\n","# Plot the learning curve\n","plt.plot(rewards)"]},{"cell_type":"markdown","metadata":{"id":"1NbkDcyfZO14"},"source":["Train your own agent!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bdc2VpjA71Kw"},"outputs":[],"source":["env = CliffBoxGridWorld()\n","agent = RLAgent(env, num_episodes=1000)\n","rewards = agent.learn()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Smooth plot\n","weight=0.99\n","last = rewards[0]\n","smoothed = []\n","for v in rewards:\n","    smoothed_val = last * weight + (1 - weight) * v\n","    smoothed.append(smoothed_val)\n","    last = smoothed_val\n","\n","# Plot the learning curve\n","plt.plot(smoothed)"]},{"cell_type":"markdown","metadata":{"id":"5dl7JgziZROA"},"source":["You need to complete the following method to visualize your training results."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1676463862962,"user":{"displayName":"Longtao Zheng","userId":"17721972102166804099"},"user_tz":-480},"id":"KiSqtBuL73Rf"},"outputs":[],"source":["def visualize(q_table):\n","    pass\n","    # TODO: Visualize learned V-table and policy.\n","    # for ... in q_table.items():\n","    #     v_table[key[0]].append(np.mean(list(value.values())))\n","    #     policy[key[0]].append(actions[np.argmax([value[i] for i in range(1, 5)])])\n","\n","visualize(agent.q_table)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyP9ohMoS+Jycd5V49qgyi7u","provenance":[{"file_id":"1IDOYe2na06C1FbbHae02fQ1NwoG3NUDg","timestamp":1676470026430},{"file_id":"1bRDhABxWGe-LIeWgzQl8GGkBmf9iKyuz","timestamp":1676444626866}]},"kernelspec":{"display_name":"marl","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"2372fb78870ca5ddfbb77edbd31465c7991d171e5767f3094f475408094f996f"}}},"nbformat":4,"nbformat_minor":0}
